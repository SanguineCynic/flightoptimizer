# -*- coding: utf-8 -*-
"""Another copy of fuelPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12ELKF2vlK45-BPcW6-9jhVgDeD17MfqK

# Import Libraries
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from IPython.display import display
from scipy import stats
import sklearn
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import pickle

"""# Extract Data"""

df = pd.read_csv('aviation_traffic_dataset_c70.csv')

df.head()

"""# Data Analysis"""

#df = df.drop(columns=['index', 'source'])

categorical_columns = []
numerical_columns = []

for column, dtype in df.dtypes.items():
    if dtype == 'object':
        categorical_columns.append(column)
    else:
        numerical_columns.append(column)

print("Categorical columns:", categorical_columns)
print("Numerical columns:", numerical_columns)

df.info()

df.describe()

df['co2'].describe()

correlation_matrix = df[numerical_columns].corr()
correlation_matrix

plt.figure(figsize=(7, 5))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Heatmap Showing Correlation between the Numeric Variables')
plt.show()

'''#dropping the values that are closely related to departure and arrival airport (location specific data).
df = df.drop(columns=['departure_lon', 'departure_lat', 'arrival_lon', 'arrival_lat', 'departure_country', 'arrival_country', 'departure_continent', 'arrival_continent', 'domestic'])
# dropping ask(Available Seat Kilometres) as it closley related to rpk(Revenue Passenger Kilometres)
df = df.drop(columns=['ask'])'''
# Dropping 'seats_no_est_scaling' as it basically duplicates information already captured by 'seats'.
df = df.drop(columns=['seats_no_est_scaling'])
df.info()

categorical_columns = []
numerical_columns = []

for column, dtype in df.dtypes.items():
    if dtype == 'object':
        categorical_columns.append(column)
    else:
        numerical_columns.append(column)

print("Categorical columns:", categorical_columns)
print("Numerical columns:", numerical_columns)

"""# Data Preprocessing & Feature Engineering

## Extract Null Values
"""

df.isnull().values.any()

"""## Encode catgeorical variables"""

for column in categorical_columns:
    print(column, df[column].nunique())

label_encoder = LabelEncoder()

for column in categorical_columns:
    df[column] = label_encoder.fit_transform(df[column])

df.head()

"""## Removing Outliers"""

plt.figure(figsize=(12, 6))
sns.boxplot(data=df, orient='h')  # Horizontal box plot

# Set plot title and labels
plt.title('Distribution of Data')
plt.xlabel('Values')

# Show the plot
plt.show()

# Define a function to remove outliers based on z-score
def remove_outliers(df, threshold=3):
    z_scores = np.abs(stats.zscore(df))
    df_no_outliers = df[(z_scores < threshold).all(axis=1)]
    return df_no_outliers

# Apply the function to your DataFrame
df_no_outliers = remove_outliers(df)

# Check the shape before and after removing outliers
print("Shape before removing outliers:", df.shape)
print("Shape after removing outliers:", df_no_outliers.shape)

df_no_outliers.info()

df = df_no_outliers
df.info()

"""## Train/Test Split"""

#Y = df['co2'].values
Y = df['co2'].values
df = df.drop(columns=['co2'])
X = df.values

df.info()

X

Y

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=100)
X_train.shape, X_test.shape

"""## Normalize"""

m=len(X_train)
n = len(df.columns)

X_train = X_train.reshape((m, n))
scaler = StandardScaler()
scaler.fit(X_train) # Fit the scaler to the training data

X_train = scaler.transform(X_train)  # Create the scaled TRAIN input data
X_train = X_train.reshape((m, n))

print(X_train[:5])

m=len(X_test)
X_test = X_test.reshape((m, n))

X_test = scaler.transform(X_test) # Create the scaled TEST input data
X_test = X_test.reshape((m, n))

"""# Model Training"""

from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn import linear_model

# Define a function to train the model, evaluate its performance, and get weights and bias
def evaluate_model(model, X_train, Y_train, X_test, Y_test):
    # Train the model
    model.fit(X_train, Y_train)

    # Evaluate the model on training data
    train_r2_score = model.score(X_train, Y_train)
    train_mse_score = mean_squared_error(Y_train, model.predict(X_train))
    train_mae_score = mean_absolute_error(Y_train, model.predict(X_train))

    # Evaluate the model on test data
    test_r2_score = model.score(X_test, Y_test)
    test_mse_score = mean_squared_error(Y_test, model.predict(X_test))
    test_mae_score = mean_absolute_error(Y_test, model.predict(X_test))

    # Get weights (coefficients) and bias (intercept)
    weights = model.coef_
    bias = model.intercept_

    return train_r2_score, train_mse_score, train_mae_score, test_r2_score, test_mse_score, test_mae_score, weights, bias

# Define models and assign each model to a variable
linear_reg_model = linear_model.LinearRegression()
lasso_reg_model = linear_model.Lasso(alpha=1.0)
ridge_reg_model = linear_model.Ridge(alpha=1.0)
elastic_net_model = linear_model.ElasticNet(alpha=1.0)

# Define a dictionary with model names as keys and models as values
models = {
    'Linear Regression': linear_reg_model,
    'Lasso Regression': lasso_reg_model,
    'Ridge Regression': ridge_reg_model,
    'Elastic Net Regression': elastic_net_model,
}

# Iterate over each model and evaluate its performance
for name, model in models.items():
    print(f"\nModel: {name}")
    train_r2_score, train_mse_score, train_mae_score, test_r2_score, test_mse_score, test_mae_score, weights, bias = evaluate_model(model, X_train, Y_train, X_test, Y_test)
    print("Training Performance:")
    print(f"R-squared score: {train_r2_score}")
    print(f"MSE: {train_mse_score}")
    print(f"MAE: {train_mae_score}")
    print("Test Performance:")
    print(f"R-squared score: {test_r2_score}")
    print(f"MSE: {test_mse_score}")
    print(f"MAE: {test_mae_score}")
    print("Weights (Coefficients):", weights)
    print("Bias (Intercept):", bias)

"""**Linear Regression** consistently exhibits the lowest MSE and MAE across both training and test data, indicating that it produces predictions that are closest to the actual values on average."""

Y_train_pred = linear_reg_model.predict(X_train)
Y_test_pred = linear_reg_model.predict(X_test)

train_df = pd.DataFrame({'Actual Values': Y_train, 'Predicted Values': Y_train_pred})

# Combine actual and predicted values into a DataFrame for the test data
test_df = pd.DataFrame({'Actual Values': Y_test, 'Predicted Values': Y_test_pred})

# Display the DataFrames
print("Training Data:")
display(train_df.head(30))
print("\nTest Data:")
display(test_df.head(30))

"""# Predict on More Test Data"""

test_df = pd.read_csv('aviation_traffic_dataset_c30.csv')

test_df.head()

"""## Handle Missing Values"""

test_df.isna().sum()

"""## Encode Categorical Columns"""

test_df.info()

categorical_columns

label_encoder = LabelEncoder()

for column in categorical_columns:
    test_df[column] = label_encoder.fit_transform(test_df[column])

test_df.head()

y_actual_test = test_df['co2'].values

y_actual_test

test_df = test_df.drop(columns=['co2'])

test_df

Test_X = test_df.values

Test_X

Test_X.shape

Test_X = scaler.transform(Test_X)
y_pred_test = linear_reg_model.predict(Test_X)

y_pred_test

"""## Results"""

df_results = pd.DataFrame({'Y Actual':y_actual_test, 'Y Predicted': y_pred_test})

# Calculate absolute difference
absolute_difference = np.abs(y_actual_test - y_pred_test)

# Calculate squared difference
squared_difference = (y_actual_test - y_pred_test) ** 2

# Add the columns to df_results
df_results['Absolute Difference'] = absolute_difference
df_results['Squared Difference'] = squared_difference

# Display the updated DataFrame
print(df_results.head())

df_results

df_results.to_csv('co2PredictionsNew.csv', index=False)

"""# Create Pickle File"""

# Make pickle file of our model
pickle.dump(linear_reg_model, open("fuel_Pred_Model.pkl", "wb"))